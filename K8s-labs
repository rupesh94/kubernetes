Kuberntes Essential
============================================================
=============================================================
Lab 1: Kubernetes Operations(kops) on AWS
=============================================================

##Task 1: Launching an EC2 Instance
#create an EC2 Instance Ubuntu Server22.04 LTS (HVM), SSD Volume type t2.micro

##Task 2: Create an IAM role
Create an IAM role named "kops-admin-role" with the "AdministratorAccess" policy attached.
Now, associate the IAM role "kops-admin-role" with your EC2 instance named "kops" by following these steps: ** 
Navigate to the EC2 console and select the "kops" instance. ** Click on "Action" > "Security" > "Modify IAM role." 
** Search for the "kops-admin-role" role, select it, and click "Update IAM role."

##Now, attach the IAM kops-admin-role to your EC2 instance kops by following these steps:
1. First select EC2 instance kops.
2. Next click on Action > Security > Modify IAM role
3. Search   for kops-admin role, select it and click Update IAM role

##Task 3: Setting up a Kubernetes Cluster

sudo hostnamectl set-hostname kops
bash

create file  vi kops.sh  and copy the entire content from the kops.sh file which is present in the same link.
save the file :wq and run the command chmod 400 kops.sh 
chmod 400 kops.sh

. ./kops.sh

kops get cluster

kops validate cluster <cluster name>

kubectl get nodes

kops export kubeconfig --admin
// export KOPS_STATE_STORE=s3://

#For delete your entire cluster 
=====================================
example:  kops delete cluster --name=martuj-m1-k8s-2024-06-02-03-06.k8s.local --yes

LAB-2 creating pod
================================
Task-1  Imperative method
----------
 kubectl run test --image=nginx:latest --port 80

kubectl get pod
alias k=kubectl
k get pod
k get pod -o wide
k get nodes
k describe pod test

task-2 declarative method
-----------------------------
 vi mypod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx-container
    image: nginx


k create -f mypod.yaml
kubectl get pod
alias k=kubectl
k get pod
k get pod -o wide
k get nodes
k describe pod nginx-pod

kubectl api-resources
k get po -n kube-system
k delete pod nginx-pod
k delete pod nginx-pod
=============================================================
Task-3 ReplicaSet
----------------------------
vi nginx-rs.yaml 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

kubectl create -f nginx-rs.yaml
kubectl  get pods
kubectl get pod -l app=nginx
kubectl get rs nginx-rs -o wide
kubectl  describe rs nginx-rs
kubectl get pod
kubectl delete pod nginx-rs-764mq
kubectl get pod
kubectl get rs
kubectl delete rs nginx-rs
kubectl get rs
kubectl get po
=============================================================
Lab 3: Deployment
=============================================================
----------------------------------------------------------------------
Task 1: Write a Deployment yaml and Apply it
----------------------------------------------------------------------
#Create a dep-nginx.yaml using content given below

vi dep-nginx.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-dep
  labels:
    app: nginx-dep
spec:
  replicas: 3
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx-ctr
        image: nginx:1.11
        ports:
        - containerPort: 80



#Apply the Deployment yaml created in the previous step

kubectl apply -f dep-nginx.yaml


#View the objects created by Kubernetes, Deployment and Replica Set 

kubectl get deployments
kubectl get rs


#Access one of the Pods and view nginx version

kubectl get pods
kubectl exec -it <pod_name> -- /bin/bash


# nginx -v
# exit


-------------------------------------------------------------------------
 Task 2: Update the Deployment with a Newer Image
-------------------------------------------------------------------------

#Update the nginx image in Pod using below

kubectl set image deployment/nginx-dep nginx-ctr=nginx:1.12.2


#Describe the deployment and see that the old pods are replaced with newer ones

kubectl describe deployments


#Access one of the Pods and view nginx version

kubectl get pods
kubectl exec -it <pod_name> -- /bin/bash

# nginx -v
# exit



-----------------------------------------------------------------------------
Task 3: Rollback of Deployment 
-----------------------------------------------------------------------------

#View the history of Deployments

kubectl rollout history deployment/nginx-dep


#Rollback the Deployment done in the previous task

kubectl rollout undo deployment/nginx-dep --to-revision=1

kubectl get rs


#Access one of the Pods and view nginx version

kubectl get pods
kubectl exec -it <pod_name> -- /bin/bash

# nginx -v
# exit



------------------------------------------------------------------------------
Task 4: Scaling of Deployments
------------------------------------------------------------------------------

#View the number of Pod replicas created by the Deployment

kubectl get deployments
kubectl get pods


#Scale up the deployment to have 8 Pod replicas

kubectl scale deployment nginx-dep --replicas=8



#Check the Pods and deployment to and verify that the number of Pod replicas are 8

kubectl get deployments
kubectl get pods


#Scale down the deployments to 2 Pod replicas

kubectl scale deployment nginx-dep --replicas=2


#Check the Pods and deployment to and verify that the number of Pod replicas are down to 2

kubectl get deployments
kubectl get pods

-----------------------------------------------------------------------------
#Task 5: Deployment Auto Scaling HPA â€“ Horizontal Pod
Autoscaling

kubectl autoscale deployment nginx-dep --min=3 --max=8 --cpu-percent=70
kubectl get hpa
----------------------------------------------------------------------------
#Task 6 Cleanup the resources using below command
-----------------------------------------------------------------------------
kubectl delete -f dep-nginx.yaml

---------------------------------------------------------------------------------
=============================================================
Lab 4: Services in Kubernetes
=============================================================
---------------------------------------------------------------
# Task 1 Create a pod using below yaml
---------------------------------------------------------------

vi httpd-pod.yaml


apiVersion: v1
kind: Pod
metadata:
  name: httpd-pod
  labels:
    env: prod 
    type: front-end
    app: httpd-ws
spec:
  containers:
  - name: httpd-container
    image: httpd
    ports:
       - containerPort: 80


# Apply the pod definition yaml

kubectl create -f httpd-pod.yaml


# Check the newly created Pod

kubectl get pods

kubectl get pods -o wide

# Describe Pod using below command

kubectl describe pod httpd-pod


----------------------------------------------------------------------
#Task 2  Setup ClusterIP service
----------------------------------------------------------------------

# Create  a ClusterIP service using below YAML

vi httpd-svc.yaml


apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    env: prod
    type: front-end
    app: httpd-ws
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP


 #Apply above definition using below to create a ClusterIP service

kubectl apply -f httpd-svc.yaml

 # Describe the service and verify it has populated the endpoints with IP address matching Pod label

kubectl get svc

kubectl describe svc httpd-svc

 #SSH to one of the machines and rerun the command in the previous task

 curl <Cluster_IP>:<Service_Port>


------------------------------------------------------------------------------
#Task 3  Setup NodePort Service
------------------------------------------------------------------------------

# Modify the service created in the previous task to type NodePort

vi httpd-svc.yaml

apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    env: prod
    type: front-end
    app: httpd-ws
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: NodePort


 #  Apply the changes using below command

kubectl apply -f httpd-svc.yaml


 # View details of the modified service

kubectl describe svc httpd-svc

 # Validate connectivity using External IP on NodePort using below or via browser

curl <EXTERNAL-IP>:NodePort

 # Get External IPs of the machines in the cluster. SSH to one of the machines and rerun the command in the previous task

kubectl get nodes -o wide | awk '{print $7}'

curl <Cluster_IP>:<Service_Port>
------------------------------------------------------------------------------------
#Task 4  Setup LoadBalancer Service
------------------------------------------------------------------------------------

 # Modify the service created in the previous task to type LoadBalancer 

vi httpd-svc.yaml

apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    env: prod
    type: front-end
    app: httpd-ws
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: LoadBalancer


 #  Apply the changes using below command

kubectl apply -f httpd-svc.yaml


 # Verify that a new service of type LoadBalancer has been created

kubectl get svc

kubectl describe svc httpd-svc

 
 # Access the LoadBalancer on the kops instance or via browser

curl <LoadBalancer_DNS>



-------------------------------------------------------------------------------
#Task 5 Delete and recreate httpd Pod
-------------------------------------------------------------------------------
# Delete the existing httpd-pod using below

kubectl delete -f httpd-pod.yaml

# View the service details and notice that the Endpoints field is empty

kubectl describe svc httpd-svc

# Recreate the httpd Pod and view service details Verify that the endpoints is updated with new Pod IP

kubectl apply -f httpd-pod.yaml

kubectl describe svc httpd-svc



--------------------------------------------------------------------------------
#Task 6 Cleanup the resources using below command
----------------------------------------------------------------------------------

kubectl delete -f httpd-pod.yaml
kubectl delete -f httpd-svc.yaml
==========================================================
=============================================================
Lab 5: DaemonSet in Kubernetes
=============================================================

#Create a DaemonSet using below yaml

vi ds-pod.yaml
----------------------------------------------
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: redis-ds
  labels:
    app: redis-ds
spec:
  selector:
    matchLabels:
      app: redis-app
  template:
    metadata:
       labels:
           app: redis-app
    spec:
      containers:
      - name: redis-ctr
        image: redis

-----------------------------------------------

#Apply the yaml definition to create a fluent-ds DaemonSet

kubectl apply -f ds-pod.yaml

#Check the available daemonsets in kubernetes cluster

kubectl get ds redis-ds


#Verify that pods for fluent are created one for each node using DaemonSet

kubectl get pods -o wide

#Cleanup the DaemeonSet using below 

kubectl delete -f ds-pod.yaml

#Verify the pods to find (all) the fluentd pods being deleted from each of the nodes

kubectl get pods
===========================================================================================
=============================================================
Lab 6: Persistent Volume in Kubernetes
=============================================================
Volumes
-------------------
Task:1-EmptyDir
------------------

vi emptydir.yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: test-pod
  name: test-pod
spec:
  volumes:
  - name: volume-1
    emptyDir: {}                # /var/lib/kubelet under this stores the data uder node where the pod is creating
  containers:
  - image: nginx
    name: ng-ctr
    ports:
    - containerPort: 80
    volumeMounts:
    - name: volume-1
      mountPath: /app

save the file
kubectl create -f emptydir.yaml
kubectl get pods
kubectl get pods -o wide
#Go insdie the pod and create file in the /app dir
kubectl exec -it test-pod  -- bash
cd /app
touch f1 f2
ls
exit
# now you are back to kops server
kubectl delete pod test-pod
kubectl get pod

Task:2-HostPath
=================
# Create file
vi  nginxpod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: ws
  name: host-pod
spec:
  nodeName: i-006f490d250f417ca
  volumes:
  - name: hostpath-volume
    hostPath:
      path: /pvdir
  containers:
  - image: nginx
    name: ng-ctr
    ports:
    - containerPort: 80
    volumeMounts:
    - name: hostpath-volume
      mountPath: /usr/share/nginx/html/
save the file

 kubectl create -f nginxpod.yaml
 k get pod
 kubectl expose pod host-pod --port 80 --type NodePort --name host-svc
 k get svc
 kubectl describe svc
 # take node ip and port no to access the website

#Create another pod to access the same volume
--------------------------------------------------
vi httppod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: httpd
  name: httpd-pod
spec:
  nodeName: i-006f490d250f417ca
  volumes:
  - name: httpd-volume
    hostPath:
      path: /pvdir
  containers:
  - image: httpd
    name: httpd-ctr
    ports:
    - containerPort: 80
    volumeMounts:
    - name: httpd-volume
      mountPath: /usr/local/apache2/htdocs/

save the file
 kubectl create -f httppod.yaml
 kubectl get pod
 kubectl exec -it httpd-pod  -- bash
 ls 
 cd htdocs
 cat index.html

# you can see the same data in the file
exit
kubectl get pod
kubectl delete pod  host-pod
kubectl delete pod httpd-pod
kubectl get pod
----------------------------------------------------------------------------
# Task 1  Get Node Label and Create Custom Index.html on Node
----------------------------------------------------------------------------

# View worker nodes and their labels

kubectl get nodes --show-labels | grep role=node


# make a note of the kubernetes.io/hostname label of one of the nodes


# ssh to one of the nodes using below

 ssh -t ubuntu@<node_public_IP> 



# Switch to root and run the following commands. A directory with custom index.html is created for PersistentVolume mount 

sudo su
mkdir /pvdir
echo Hello World! > /pvdir/index.html


--------------------------------------------------------------------------------
# Task 3 - Create a Local Persistent Volume
--------------------------------------------------------------------------------

vi pv-volume.yaml


kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/pvdir"



kubectl apply -f pv-volume.yaml

kubectl get pv

kubectl describe pv pv-volume

------------------------------------------------------------------------------------
# Task 4  - Create a PV Claim
------------------------------------------------------------------------------------
vi pv-claim.yaml


kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv-claim
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

kubectl apply -f pv-claim.yaml
----------------------------------------------------------------------------------------
# Task 5  - Create nginx Pod with NodeSelector
----------------------------------------------------------------------------------------
vi pv-pod.yaml


kind: Pod
apiVersion: v1
metadata:
  name: pv-pod
spec:
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: pv-claim
  containers:
     - name: pv-container
       image: nginx
       ports:
          - containerPort: 80
            name: "http-server"
       volumeMounts:
          - mountPath: "/usr/share/nginx/html"
            name: pv-storage
  nodeSelector:
    kubernetes.io/hostname: ip-172-20-33-138.ap-south-1.compute.internal

# Apply the Pod yaml created in the previous step

kubectl apply -f pv-pod.yaml

# View Pod details and see that is created on the required node

kubectl get pods -o wide

# Access shell on a container running in your Pod

kubectl exec -it pv-pod -- /bin/bash

# Run the following commands in the container to verify PersistentVolume

 apt-get update
 apt-get install curl -y
 curl localhost
 exit

# delete the resources created in this lab.
kubectl delete -f pv-pod.yaml
kubectl delete -f pv-claim.yaml
kubectl delete -f pv-volume.yaml
=============================================================
---------------------------------------------
